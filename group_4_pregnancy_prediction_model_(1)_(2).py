# -*- coding: utf-8 -*-
"""Group_4_Pregnancy_prediction_model (1) (2).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1gmWPeoHmatbT0uoRugDgvq2otccjP83O

# Building a Classification model

## Problem Statement

Polycystic ovary syndrome is a disorder involving infrequent, irregular or prolonged menstrual periods, and often excess male hormone (androgen) levels. The ovaries develop numerous small collections of fluid — called follicles — and may fail to regularly release eggs.

PCOS(Polycystic ovary syndrome) is becoming a common condition in women in the modern world. Also, it is believed that women with PCOS problem suffer with weight gain, pregnancy issues, hairfall, and so on and it might affect women's fertility rate.

In this project, we are building a classification model to predict if a women can get pregnant based on multiple features.

## Success Criteria

**Business Success Criteria:** Reduce the diagnosis time from anywhere between 20% to 40%

**ML Success Criteria:** Achieve Accuracy of atleast 0.8

## Data Collection

Data: Dataset contains all physical and clinical parameters to determine PCOS and infertility related issues. The data are collected from 10 different hospital across Kerala,India.

author = {Prasoon Kottarathil},
title = {Polycystic ovary syndrome (PCOS)},
year = {2020},
publisher = {kaggle},
journal = {Kaggle Dataset},
how published = {\url{https://www.kaggle.com/prasoonkottarathil/polycystic-ovary-syndrome-pcos}}

**Importing required packages**
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler

from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import confusion_matrix

df = pd.read_excel('PCOS_data_without_infertility.xlsx',sheet_name=1)

df.head(10)

df.columns

"""## EXPLORATORY DATA ANALYSIS (EDA) / DESCRIPTIVE STATISTICS"""

df.describe().T

df.info()

"""##  Data preprocessing

### Typecasting

**changing all the (Y/N) encoded variables to object datatype**

**Blood group is a categorical variable with different blood groups, typecasting it to objects datatype**

**Converting Sl. No, Patient File No., and no. of abortions as objects since they are unique values and not integers**
"""

for i in df.columns:
    if "(Y/N)" in i :
        df[i]=df[i].astype('object')

df['Blood Group']=df['Blood Group'].astype('object')

df['Sl. No']=df['Sl. No'].astype('object')
df['Patient File No.']=df['Patient File No.'].astype('object')
df['No. of aborptions']=df['No. of aborptions'].astype('object')
df['Cycle length(days)']=df['Cycle length(days)'].astype('object')

"""**Converting columns which are misinterpreted as objects due to junk values to numeric**"""

def to_numeric(input_col):
    return pd.to_numeric(input_col,errors='coerce')

df["II    beta-HCG(mIU/mL)"] = to_numeric(df["II    beta-HCG(mIU/mL)"])
df["AMH(ng/mL)"] = to_numeric(df["AMH(ng/mL)"])

"""### Datatypes after preprocessing"""

df.dtypes

no_of_records = df.shape[0]
no_of_records

"""### Setting the target variable as 'Pregnant'"""

target = 'Pregnant(Y/N)'

"""### Dropping the columns with 90% of missing data"""

null_col_dict = dict([(i,sum(df[i].isnull())) for i in df.columns])
null_col_dict

columns_removed = []
for i in null_col_dict:
    if null_col_dict[i] >= 0.90*no_of_records:
        df=df.drop(i,axis=1)
        columns_removed.append(i)

columns_removed

"""### Code to remove zero variance numerical variables"""

numerical_col = list(set(df.select_dtypes(exclude='object'))-set(target))
categorical_col = list(set(df.select_dtypes(include='object'))-set(target))



zero_var_numerical_col = df.std()[round(df.std(),3)==0]
df = df.drop(zero_var_numerical_col.index,axis=1)

numerical_col = list(set(numerical_col)-set(zero_var_numerical_col))
zero_var_numerical_col

"""### Code to remove zero variance categorical variables"""

zero_var_categorical_col = [i for i in categorical_col if len(df[i].value_counts().index)==1]
df=df.drop(zero_var_categorical_col,axis=1)

categorical_col = list(set(categorical_col)-set(zero_var_categorical_col))
zero_var_categorical_col

multi_var_categorical_col = [i for i in categorical_col if len(df[i].value_counts().index)>=200]
df=df.drop(multi_var_categorical_col,axis=1)

categorical_col = list(set(categorical_col)-set(multi_var_categorical_col))
multi_var_categorical_col

len(numerical_col)

def find_outliers_IQR(df):
    q1=df.quantile(0.25)
    q3=df.quantile(0.75)
    IQR=q3-q1
    outliers = df[((df<(q1-1.5*IQR)) | (df>(q3+1.5*IQR)))]
    return outliers

outlier_dict={}
for i in numerical_col:
    outliers = find_outliers_IQR(df[i])
    #if len(outliers)>0:

    if len(outliers)> 0.03*(len(df[i])):
        outlier_dict[str(i)] = len(outliers)

print(len(outlier_dict))
print(outlier_dict)
out_list = list(outlier_dict.keys())

outliers = find_outliers_IQR(df[' Age (yrs)'])
len(outliers)

"""### Number of women who has pcos and got pregnant"""

a = df[((df.loc[:,'PCOS (Y/N)']==1) & (df.loc[:,target]==1))].shape[0]
a

"""### Number of women who has pcos and did not get pregnant"""

b = df[((df.loc[:,'PCOS (Y/N)']==1) & (df.loc[:,target]==0))].shape[0]
b

"""### Data Visualization"""

df.columns

"""After consulting a practicing gynaecologist, we concluded that the important fields for the pregancy prediction model are the following :-



**PCOS,BMI,
Weight,
Cycle length,
FSH/LH,
Waist:Hip Ratio,
Weight gain(y/n),
hair growth(Y/N),
Pimples(Y/N),
Fast food (Y/N),
Reg.Exercise(Y/N),
Follicle No. (L),
Follicle No. (R),
Avg. F size (L) (mm),
Avg. F size (R) (mm)**


"""

for i in ['Weight gain(Y/N)', 'hair growth(Y/N)', 'Hair loss(Y/N)', 'PCOS (Y/N)','Reg.Exercise(Y/N)','Pimples(Y/N)','Cycle length(days)']:
    sns.set(style = 'darkgrid')
    plt.figure()
    ax= sns.countplot(x = i, hue = 'Pregnant(Y/N)',palette="Pastel2", data = df)

plt.figure(figsize=(30, 30))
dataplot = sns.heatmap(df.corr(), cmap=None, annot=True)
plt.show()

#Percentage of Pregnant and not pregnant people with PCOS
fig1, ax1 = plt.subplots()
x = list(df['Pregnant(Y/N)'].value_counts().index)
y = list(df[df['PCOS (Y/N)'] == 1]['Pregnant(Y/N)'].value_counts().values)
ax1.pie(y, labels = x, autopct= '%1.2f%%', colors = "cyan", radius= 1.2, startangle = 90)
plt.show()

#Percentage of Pregnant and not pregnant people without PCOS
fig1, ax1 = plt.subplots()
x = list(df['Pregnant(Y/N)'].value_counts().index)
y = list(df[df['PCOS (Y/N)'] == 0]['Pregnant(Y/N)'].value_counts().values)
ax1.pie(y, labels = x, autopct='%1.2f%%', colors = "cyan", radius= 1.2, startangle = 90)
plt.show()

#Visualising cycle length and AMH(Anti-Mullerian Hormone) w.r.t PCOS
ax = sns.scatterplot(x= 'Follicle No. (L)', y = 'AMH(ng/mL)' , palette="CMRmap", hue = 'Pregnant(Y/N)', data = df)

#Visualizing the patterns of target variable Pregnant(Y/N) with increase in BMI and waist-hip ratio
ax = sns.scatterplot(x= 'BMI', y =  'Waist:Hip Ratio', hue = 'Pregnant(Y/N)', palette="CMRmap", data = df)

#Plotting No. of Follicle w.r.t PCOS
ax = sns.scatterplot(y= 'Follicle No. (L)', x = 'Follicle No. (R)', hue = 'Pregnant(Y/N)', palette="CMRmap", data = df)

"""### Normalization"""

from sklearn import preprocessing
array = df[numerical_col].values
data_scaler = preprocessing.MinMaxScaler(feature_range = (0,1))
df[numerical_col] = pd.DataFrame(data_scaler.fit_transform(array), columns = numerical_col)

"""### Imputing the mode for missing categorial variables and mean for missing numerical variables

#### Before Imputing
"""

null_col_dict = dict([(i,sum(df[i].isnull())) for i in df.columns])
null_col_dict

"""### Using SimpleImputer from sklearn to impute categorical variables"""

from sklearn.impute import SimpleImputer
imputer = SimpleImputer(strategy='most_frequent',
                        missing_values=np.nan)
imputer = imputer.fit(df[categorical_col])
df[categorical_col] = imputer.transform(df[categorical_col])

"""### Using fillna function to impute numerical variables"""

df[numerical_col]=df[numerical_col].fillna(df[numerical_col].mean(), inplace = False)

"""#### After Imputing"""

null_col_dict = dict([(i,sum(df[i].isnull())) for i in df.columns])
null_col_dict

"""### Balancing Dataset"""

df[target].value_counts()

df[target]=df[target].astype('uint8')

from imblearn.over_sampling import SMOTE

input_var = list(set(df.columns) - set([target]))

over_sam = SMOTE(random_state=0)
X,Y = over_sam.fit_resample(df[input_var],df[target])

X = pd.DataFrame(X, columns = input_var)
y = pd.DataFrame(Y, columns = [target])

y[target].value_counts()

# your code to create train and test sets goes in here
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.30,random_state=40)

from sklearn.linear_model import LogisticRegression
from sklearn.feature_selection import SelectFromModel

sel_ = SelectFromModel(LogisticRegression(C=1, penalty='l2', solver='liblinear'))
sel_.fit(X_train, np.ravel(y_train,order='C'))

selected_feat = X_train.columns[(sel_.get_support())]
selected_feat= list(selected_feat)

selected_feat.append('PCOS (Y/N)')

selected_feat

X_train_selected = X_train[selected_feat]
X_test_selected = X_test[selected_feat]
X_train_selected.shape, X_test_selected.shape

models = ['LogisticRegression', 'DecisionTreeClassifier', 'RandomForestClassifier', 'KNN']

# Logistic Regression
log_reg = LogisticRegression(random_state=0, solver='lbfgs', multi_class='ovr') # creates a lR instance
log_reg.fit(X_train_selected, y_train)

# Decision Trees
dec_tree = DecisionTreeClassifier(criterion = 'gini', splitter='best', max_depth=15)
dec_tree.fit(X_train_selected, y_train)

# Random Forests
rf = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=0)
rf.fit(X_train_selected, y_train)

# K-NN
knn = KNeighborsClassifier(n_neighbors = 5)
knn.fit(X_train_selected, y_train)

def get_performance_metrics(actual,predict):
    c_matrix = confusion_matrix(actual,predict)
    total = sum(sum(c_matrix))
    accuracy = (c_matrix[0,0]+c_matrix[1,1])/total
    sensitivity = c_matrix[0,0]/(c_matrix[0,0]+c_matrix[0,1])
    specificity = c_matrix[1,1]/(c_matrix[1,0]+c_matrix[1,1])
    dict_ = {"accuracy":accuracy,"sensitivity":sensitivity,"specificity":specificity}
    return dict_

"""### Predict the target variable"""

predict_lr=log_reg.predict(X_test_selected)
predict_dt=dec_tree.predict(X_test_selected)
predict_rf=rf.predict(X_test_selected)
predict_knn=knn.predict(X_test_selected)

# accuracy, sensitivity, and specificity for model logistic regression
performance_lr = get_performance_metrics(y_test, predict_lr)


# accuracy, sensitivity, and specificity for model decision trees
performance_dt = get_performance_metrics(y_test, predict_dt)


# accuracy, sensitivity, and specificity for model random forests
performance_rf = get_performance_metrics(y_test, predict_rf)

# accuracy, sensitivity, and specificity for model k nearest neighbors
performance_knn = get_performance_metrics(y_test, predict_knn)

perf_df = pd.DataFrame([performance_lr,performance_dt,performance_rf,performance_knn],index = ['Logistic Regression',
                             'Decision Trees', 'Random Forest','K-NN'])
perf_df.round(2)